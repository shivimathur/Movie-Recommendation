# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SoJKuc_kyAxDBWUOUQc-3bCe9k2sxalM
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Importing necessary libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
import re
from tensorflow import keras
import operator
import warnings
warnings.filterwarnings('ignore')
pd.set_option('expand_frame_repr', False)

"""#Reading the data files"""

ratings=pd.read_csv("/content/drive/MyDrive/PRML LAB/PRML Project/ratings.csv")
ratings

movies=pd.read_csv("/content/drive/MyDrive/PRML LAB/PRML Project/movies.csv")
movies

links=pd.read_csv("/content/drive/MyDrive/PRML LAB/PRML Project/links.csv")
links

tags=pd.read_csv("/content/drive/MyDrive/PRML LAB/PRML Project/tags.csv")
tags

"""#Data Visualisation"""

#function to apply nlp on tags
def nlp(): 
  punc='''!.,'"/[]{}()^&*$%#@_~;:<>?\|`'''
  tags_copy=tags.copy()

  for i in tags_copy.index:
    str1=tags_copy.loc[i,"tag"]
    str_new=""
    for char in str1:
      if(char not in punc):
        str_new+=char
    str_new=str_new.strip()
    tags_copy.at[i,"tag"]=str_new

  #drop the timestamp column
  tags_copy=tags_copy.drop("timestamp", axis=1).dropna()
  tags_df=tags.copy().drop("timestamp", axis=1).dropna()
  tags_df["tag"]=tags_df["tag"].str.lower()

  #use re library to remove the punctuation marks 
  for i in tags_df.index:
    tag_name=tags_df.loc[i,"tag"]
    edited_tag=re.sub(r' \([^)]*\)','',tag_name)

    #change the tag sci-fi to edited tag because later on we will remove all the 2 letters words
    if "-" in edited_tag:
      tags_df.at[i,"tag"]=edited_tag

    #change the tag starting with "based on a" to edited tag because later on we will remove all the 1 letter words
    if "based" in edited_tag:
      tags_df.at[i,"tag"]=edited_tag

    #change all the 1 letter words to null values
    if re.findall(r'\b\w{1}\b', edited_tag):
      tags_df.at[i,"tag"]=np.NaN
    #change all the 2 letter words to null values
    elif re.findall(r'\b\w{2}\b', edited_tag):
      tags_df.at[i,"tag"]=np.NaN
    else:
    #change the tag to edited tag
      tags_df.at[i,"tag"]=edited_tag

  #delete the null values
  tags_df=tags_df.dropna()
  return tags_df

#function to make and sort a dictionary of unique tags and their count
def sorted_dictionary(tags_df):
  unique_tags={}
  #make the dictionary 
  for i in tags_df.index:
    tag_name=tags_df.loc[i,"tag"]
    if tag_name in unique_tags.keys():
      unique_tags[tag_name]+=1
    else:
      unique_tags[tag_name]=1

  #sort the dictionary 
  marklist=sorted(unique_tags.items(), key=lambda x:x[1])
  sortdict=dict(marklist)
  return sortdict #returning the dictionary 

tags_df=nlp()
sortdict=sorted_dictionary(tags_df)

#plotting number of tags with given unique value count
def tags_visualization_1(tags_df, sortdict):
  unique_tags_dict={}
  for k in sortdict:
    if sortdict[k] in unique_tags_dict:
      unique_tags_dict[sortdict[k]]+=1
    else:
      unique_tags_dict[sortdict[k]]=1
  plt.rcParams["figure.figsize"]=[15,8]
  tags=list(unique_tags_dict.keys())
  values=list(unique_tags_dict.values())
  plt.bar(tags, values, color ="green", width=0.4)
  plt.title("number of tags with given unique value count")
  plt.xlabel("number of tags")
  plt.ylabel("unique value count")

tags_visualization_1(tags_df, sortdict)

#plotting the tags with maximum unique counts
def tags_visualization_2(tags_df, sortdict):
  tags=list(sortdict.keys())
  values=list(sortdict.values())
  tags=tags[::-1]
  values=values[::-1]
  tags_new=[]
  values_new=[]

  for i in range(20):
    tags_new.append(tags[i])
    values_new.append(values[i])
  plt.rcParams["figure.figsize"]=[24,8]
  plt.bar(tags_new, values_new, color ='maroon', width=0.4)
  plt.title("tags with maximum unique counts")
  plt.xlabel("tags")
  plt.ylabel("unique value counts")

tags_visualization_2(tags_df, sortdict)

#identify the unique genres in the dataset
unique_genres=[]
for i in movies.index:
  genres=movies.loc[i,"genres"].split('|')
  unique_genres.extend(genres)
unique_genres=list(set(unique_genres))
unique_genres.remove("(no genres listed)")
unique_genres.remove("IMAX")
unique_genres.append("None")

#make a list of zeroes to store count of each genre
zero_list=np.zeros(9742)
data={
    'movieId' : zero_list,
    'Action' : zero_list,
    'Fantasy' : zero_list, 
    'Crime' : zero_list, 
    'Documentary' : zero_list, 
    'Western' : zero_list, 
    'Children' : zero_list, 
    'Animation' : zero_list, 
    'Adventure' : zero_list, 
    'Musical' : zero_list, 
    'Sci-Fi' : zero_list, 
    'War' : zero_list, 
    'Horror' : zero_list, 
    'Mystery' : zero_list, 
    'Drama' : zero_list, 
    'Romance' : zero_list, 
    'Thriller' : zero_list, 
    'Film-Noir' : zero_list, 
    'Comedy' : zero_list, 
    'None' : zero_list
}

#form a dataframe that relates movies to genres
genres_df=pd.DataFrame(data)
for i in movies.index:
  genres_df.at[i,"movieId"]=movies.loc[i,"movieId"]
  genres=movies.loc[i,"genres"].split('|')
  for j in range(len(genres)):
    if(genres[j]=="(no genres listed)"):
      genres_df.at[i,"None"]=1
    elif(genres[j]!="IMAX"):
      genres_df.at[i,genres[j]]=1

movie=movies.drop(columns=["genres"])
df1=movie
df1=df1.merge(genres_df,on="movieId",how="right")
items_dataset=df1
dataset=ratings
movie_dataset=items_dataset[['movieId','title']]
merged_dataset=pd.merge(dataset, movie_dataset, how='inner', on='movieId')




count=[]
for i in unique_genres:
  genre_based_movies=items_dataset[['movieId','title',i]]
  genre_based_movies=genre_based_movies[genre_based_movies[i] == 1]
  count.append(len(genre_based_movies))

#plot the data
df=pd.DataFrame({'Movie genre':unique_genres, 'Number of movies':count})
ax=df.plot.bar(x='Movie genre', y='Number of movies', rot=60, figsize=(10, 5))

"""#Recommendations

##Recommendations using Correlation
"""

def prediction_using_correlation(movie_name):
  
  #merge the datasets
  df=ratings
  tags1=tags.drop(columns=['userId','timestamp'])
  df=df.merge(movies,on='movieId',how='left')
  df=df.merge(tags1,on='movieId',how='left')
  
  #calculate average rating of each movie
  rating=pd.DataFrame(df[['title','rating']].groupby('title')['rating'].mean())

  #claculating the number of people that rated a particular movie
  rating['number of rates']=df.groupby('title')['rating'].count()
  rating=pd.DataFrame(rating)

  #evaluating correlation between user and movie
  movie_user=df.pivot_table(index='userId',columns='title',values='rating')
  correlation=movie_user.corrwith(movie_user[movie_name])
  recommandation=pd.DataFrame(correlation,columns=['correlation']).dropna().join(rating['number of rates'])
  recc=recommandation[recommandation['number of rates']>200].sort_values('correlation',ascending=False).reset_index().merge(movies,on='title',how='left')
  if recc.loc[0,"title"]==movie_name:
    recc=recc.drop(0)

  return recc

recc=prediction_using_correlation('Star Trek (2009)')

recc.head(10)

"""##Recommendation based on top rated movies that user has not seen"""

def recommendation_using_ratings(userid):
  dataset=ratings
  A=dataset.pivot(index="userId", columns='movieId', values='rating')
  A.head()
  A=A.fillna(0)
  A=np.array(A)
  A_df=pd.DataFrame(A)
  user_ratings_mean=np.mean(A, axis=1)
  R_demeaned=A - user_ratings_mean.reshape(-1, 1) #demean the data to centralise and normalise it

  def MatrixFactor(k,learning_rate,num_iter,A):

    nan_cell_mask=np.where(np.isnan(A))
    A[nan_cell_mask]=-1
    U=np.random.rand(np.size(A, 0), k)
    VT=np.random.rand(k, np.size(A, 1))

    loss_list=np.array([])
    for n in range(num_iter):
        
        for i in range(len(U)):

            # calcualte the squared Frobenius norm 
            A_est=np.matmul(U, VT)
            diff_error=np.subtract(A, A_est)
            diff_error[nan_cell_mask]=0
            sq_error=np.square(diff_error) 
            sq_error_sum=0.5 * sq_error.sum() # the ignore missing data


            # calculate the gradient for each entry of U
            grad_U=-np.matmul(diff_error, np.transpose(VT)) 
            # calculate the gradient for each entry of VT
            grad_VT=-np.matmul(np.transpose(U), diff_error) 
          
            # update U and VT 
            U= np.subtract(U, learning_rate*grad_U) 
            VT=np.subtract(VT, learning_rate*grad_VT) 
            
        loss_list=np.append(loss_list, sq_error_sum)
    
    return U,VT,loss_list
  
  U,Vt,sigma=MatrixFactor(24, 0.001, 24, R_demeaned)

  sigma=np.diag(sigma)

  all_user_predicted_ratings=np.dot(np.dot(U, sigma), Vt) 
  all_user_predicted_ratings += user_ratings_mean.reshape(-1, 1)

  preds_df=pd.DataFrame(all_user_predicted_ratings, columns=A_df.columns)

  movies_df=movies
  def recommend_movies(predictions_df, userID, movies_df, original_ratings_df):

    num_recommendations=10
    
    # Get and sort the user's predictions
    user_row_number=userID - 1 # UserID starts at 1, not 0
    sorted_user_predictions=preds_df.iloc[user_row_number].sort_values(ascending=False) 
    
    # Get the user's data and merge in the movie information.
    user_data=original_ratings_df[original_ratings_df.userId == (userID)]
    user_full=(user_data.merge(movies_df, how='left', left_on='movieId', right_on='movieId').sort_values(['rating'], ascending=False))

    print ('Recommending highest {0} predicted ratings movies not already rated.'.format(num_recommendations))
    
    # Recommend the highest predicted rating movies that the user hasn't seen yet.
    recommendations=(movies_df[~movies_df['movieId'].isin(user_full['movieId'])].
         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how='left',
               left_on='movieId', right_index=True).
         rename(columns={user_row_number: 'Predictions'}).
         sort_values('Predictions', ascending=False).
                       iloc[:num_recommendations, :-1]
                      )

    return user_full, recommendations
  
  already_rated, predictions=recommend_movies(preds_df, userid, movies_df, dataset)

  return already_rated,predictions

"""##Recommendation based on Collaborative Filtering using KNN"""

def collaborative_knn(movie_name):

  def create_dataset():
    final_dataset=ratings.pivot(index='movieId',columns='userId',values='rating')
    final_dataset=final_dataset.fillna(0)
    no_user_voted=ratings.groupby('movieId')['rating'].agg('count')
    no_movies_voted=ratings.groupby('userId')['rating'].agg('count')
    final_dataset=final_dataset.loc[:,no_movies_voted[no_movies_voted > 50].index]
    csr_data=csr_matrix(final_dataset.values)
    final_dataset.reset_index(inplace=True)

    return csr_data,final_dataset
    


  def get_movie_recommendation(csr_data,final_dataset):
      knn=NearestNeighbors(metric='cosine', algorithm='auto', n_neighbors=15, n_jobs=-1)
      knn.fit(csr_data)
      n_movies_to_reccomend=10
      movie_list=movies[movies['title'].str.contains(movie_name)]
      if len(movie_list):        
          
          movie_idx=movie_list.iloc[0]['movieId']
          movie_idx=final_dataset[final_dataset['movieId'] == movie_idx].index[0]
          distances , indices=knn.kneighbors(csr_data[movie_idx],n_neighbors=n_movies_to_reccomend+1)    
          rec_movie_indices=sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]
          recommend_frame=[]
          for val in rec_movie_indices:
              movie_idx=final_dataset.iloc[val[0]]['movieId']
              idx=movies[movies['movieId'] == movie_idx].index
              recommend_frame.append({'Title':movies.iloc[idx]['title'].values[0],'Distance':val[1]})
          df=pd.DataFrame(recommend_frame,index=range(1,n_movies_to_reccomend+1))
          return df
      else:
          return "Movie not present in data set"
  movie_name=re.sub('\(.*\)', '', movie_name)
  csr_data,final_dataset=create_dataset()
  return(get_movie_recommendation(csr_data,final_dataset))

import re
predict_knn=collaborative_knn('Star Trek (2009)')

predict_knn

"""##Recommendations for a new user

###Splitting on basis of genre
"""

unique_genres=[]
for i in movies.index:
  genres=movies.loc[i,"genres"].split('|')
  unique_genres.extend(genres)
unique_genres=list(set(unique_genres))
print(unique_genres)
unique_genres.remove("(no genres listed)")
unique_genres.remove("IMAX")
unique_genres.append("None")
print(unique_genres)

zero_list=np.zeros(9742)
data={
    'movieId' : zero_list,
    'Action' : zero_list,
    'Fantasy' : zero_list, 
    'Crime' : zero_list, 
    'Documentary' : zero_list, 
    'Western' : zero_list, 
    'Children' : zero_list, 
    'Animation' : zero_list, 
    'Adventure' : zero_list, 
    'Musical' : zero_list, 
    'Sci-Fi' : zero_list, 
    'War' : zero_list, 
    'Horror' : zero_list, 
    'Mystery' : zero_list, 
    'Drama' : zero_list, 
    'Romance' : zero_list, 
    'Thriller' : zero_list, 
    'Film-Noir' : zero_list, 
    'Comedy' : zero_list, 
    'None' : zero_list
}
genres_df=pd.DataFrame(data)
genres_df

for i in movies.index:
  genres_df.at[i,"movieId"]=movies.loc[i,"movieId"]
  genres=movies.loc[i,"genres"].split('|')
  for j in range(len(genres)):
    if(genres[j]=="(no genres listed)"):
      genres_df.at[i,"None"]=1
    elif(genres[j]!="IMAX"):
      genres_df.at[i,genres[j]]=1
print(genres_df)

"""###Creating a merged dataset"""

movie=movies.drop(columns=["genres"])
movie

df1=movie
df=df1
df1

df1=df1.merge(genres_df,on="movieId",how="right")
df1

items_dataset=df1
dataset=ratings
movie_dataset=items_dataset[['movieId','title']]
movie_dataset.head()

merged_dataset=pd.merge(dataset, movie_dataset, how='inner', on='movieId')
merged_dataset.head()

"""###Recommendation based on top average rating"""

#obtain movies with top average rating
def top_avg_rating():
  avg_highly_rated_movies=merged_dataset.groupby(['title']).agg({"rating":"mean"})['rating'].sort_values(ascending=False)
  avg_highly_rated_movies=avg_highly_rated_movies.reset_index(level=0)
  avg_highly_rated_movies.columns=['title', 'avg rating']

  return avg_highly_rated_movies

avg_highly_rated=top_avg_rating()
avg_highly_rated.head(10)

"""###Recommendation based on popularity"""

#obtain most watched movies
def most_popular():
  merged_dataset.groupby(['title']).agg({"rating":"sum"})['rating'].sort_values(ascending=False)
  popular_movies=merged_dataset.groupby(['title']).agg({"rating":"count"})['rating'].sort_values(ascending=False)
  popular_movies=pd.DataFrame(popular_movies)
  popular_movies.reset_index(level=0, inplace=True)
  popular_movies.columns=['title', 'Number of Users watched']
  
  return popular_movies

popular_movies=most_popular()
popular_movies.head(10)

"""###Recommendation based on ratings and popularity"""

highly_rated_popular_movies=pd.merge(avg_highly_rated, popular_movies, how='inner', on='title')

highly_rated_popular_movies[(highly_rated_popular_movies['Number of Users watched']>200) & (highly_rated_popular_movies['avg rating']>=4.0)]

"""###Recommending the top movies of every genre"""

def recommendations_genre(genre):
  x=genre
  print("GENRE:", x)
  genre_based_movies=items_dataset[['movieId','title',x]]
  genre_based_movies=genre_based_movies[genre_based_movies[x] == 1]
  merged_genre_movies=pd.merge(dataset, genre_based_movies, how='inner', on='movieId')
  popular_movies_ingenre=merged_genre_movies.groupby(['title']).agg({"rating":"count"})['rating'].sort_values(ascending=False)
  popular_movies_ingenre=popular_movies_ingenre.to_frame()
  popular_movies_ingenre.reset_index(level=0, inplace=True)
  popular_movies_ingenre.columns=['title', 'Number of Users watched']

  high_rated_movies=merged_genre_movies.groupby(['title']).agg({"rating":"mean"})['rating'].sort_values(ascending=False)
  high_rated_movies=high_rated_movies.to_frame()
  highly_rated_popular_movies=pd.merge(high_rated_movies, popular_movies_ingenre, how='inner', on='title')
  viewer_limit=300
  ratings_limit=4.0
  count=0
  check=0
  while viewer_limit > 0 and ratings_limit > 0:
    s=highly_rated_popular_movies[(highly_rated_popular_movies['Number of Users watched']>viewer_limit) & (highly_rated_popular_movies['rating']>=ratings_limit)]
    if len(s) < 13:
      if check == 0:
        viewer_limit -= 50
        check=1
      else:
        ratings_limit -= 0.5
        check=0
    else:
      break


  print(s[['title','rating']])
  print()

def make_user_rating_dataset():
  # creating dataframe based on user id 

  zero_list=np.zeros(610)
  useridlist=[]
  for i in range(1,611):
    useridlist.append(i)
    data={
        'userId' : useridlist,
        'Action' : zero_list,
        'Fantasy' : zero_list, 
        'Crime' : zero_list, 
        'Documentary' : zero_list, 
        'Western' : zero_list, 
        'Children' : zero_list, 
        'Animation' : zero_list, 
        'Adventure' : zero_list, 
        'Musical' : zero_list, 
        'Sci-Fi' : zero_list, 
        'War' : zero_list, 
        'Horror' : zero_list, 
        'Mystery' : zero_list, 
        'Drama' : zero_list, 
        'Romance' : zero_list, 
        'Thriller' : zero_list, 
        'Film-Noir' : zero_list, 
        'Comedy' : zero_list, 
        'None' : zero_list
    }
  user_ratings_df=pd.DataFrame(data)
  user_ratings_df #construct an empty dataset

  total_ratings=np.zeros(610)
  k=0
  for i in ratings.index:
    userid=ratings.loc[i,"userId"]
    movieid=ratings.loc[i,"movieId"]
    idx=movies.index[movies["movieId"]==movieid].values
    idx=int(idx)
    genres_list=movies.loc[idx,"genres"].split('|') #make list of all the genres in a movie
    for j in range(len(genres_list)):
      if(genres_list[j]=="(no genres listed)"): #since we have changed "(no genres listed)" to "None".
        user_ratings_df.at[userid-1,"None"]+=ratings.loc[i,"rating"]
      elif(genres_list[j]!="IMAX"): #since "IMAX" is not a genre and we removed it earlier 
        user_ratings_df.at[userid-1,genres_list[j]]+=ratings.loc[i,"rating"]
      total_ratings[userid-1]+=ratings.loc[i,"rating"] #total ratings given by a user for all the genres

  k=0
  cols=['Comedy', 'Drama', 'Mystery', 'Horror', 'Action', 'Documentary', 'Film-Noir', 'Thriller', 'Fantasy', 'Adventure', 'Western', 'Crime', 'Children', 'Romance', 'Sci-Fi', 'Musical', 'War', 'Animation', 'None']
  for i in user_ratings_df.index: #divide the cell values woth total rating given by each user to obtain mean values
    for col in cols:
      user_ratings_df.at[i,col]/=total_ratings[k]
    k+=1

  return(user_ratings_df) #return the dataset 

user_ratings_df=make_user_rating_dataset()

"""##Recommendations using Deep Learning Models"""

#function to predict movies for every user based on the genres of the past movies rated
def predict_based_on_user_id(userid):
  def make_dataset(userid): # make an empty dataset which has all the movies and genres columns
    zero_list=np.zeros(9742)
    data={
        'userId' : zero_list,
        'movieId' : zero_list,
        'Action' : zero_list,
        'Fantasy' : zero_list, 
        'Crime' : zero_list, 
        'Documentary' : zero_list, 
        'Western' : zero_list, 
        'Children' : zero_list, 
        'Animation' : zero_list, 
        'Adventure' : zero_list, 
        'Musical' : zero_list, 
        'Sci-Fi' : zero_list, 
        'War' : zero_list, 
        'Horror' : zero_list, 
        'Mystery' : zero_list, 
        'Drama' : zero_list, 
        'Romance' : zero_list, 
        'Thriller' : zero_list, 
        'Film-Noir' : zero_list, 
        'Comedy' : zero_list, 
        'None' : zero_list
    }

    complete_df=pd.DataFrame(data)
    df_new=movies
    df_new=df.merge(genres_df,on="movieId",how="right") 
    genres_list=['Comedy', 'Mystery', 'Musical', 'Fantasy', 'Thriller', 'Western', 'Horror', 'Sci-Fi', 'Romance', 'Crime', 'Adventure', 'Action', 'Animation', 'Documentary', 'War', 'Drama', 'Children', 'Film-Noir', 'None']
    for i in df_new.index:
      complete_df.at[i,"userId"]=userid
      complete_df.at[i,"movieId"]=df_new.loc[i,"movieId"]
      for l in genres_list:
        complete_df.at[i,l]=user_ratings_df.loc[userid-1,l]*df_new.loc[i,l] #only take those cell values which have genre =1 

    return complete_df #return the dataset 

  final_df=user_ratings_df
  final_df=final_df.merge(ratings,on="userId",how="left")
  final_df
  final_df=final_df.drop("timestamp",axis=1)
  y=final_df["rating"]
  X=final_df.drop("rating",axis=1)


  #applying the model
  user_ratings=keras.Input(shape= (21,))
  movie_genres=keras.Input(shape= (21,))

  user_ratings_input=keras.layers.Dense(21, activation= 'relu')(user_ratings)
  user_ratings_hidden_1=keras.layers.Dense(50, activation= 'relu')(user_ratings_input)
  user_ratings_hidden_2=keras.layers.Dense(50, activation= 'relu')(user_ratings_hidden_1)

  output_rating=keras.layers.Dense(1, activation= 'sigmoid')(user_ratings_hidden_2)
  genres_model=keras.Model(inputs= [user_ratings], outputs= output_rating)
  genres_model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001), loss= 'mean_squared_error')
  genres_model.fit(X,y)

  #predict the values for all the movies for a particular user
  def predict(userid):
    df_2=make_dataset(userid)
    pred=genres_model.predict(df_2)
    return df_2, pred

  df_2,pred=predict(3)
  
  #dictionary which contains movies seen by a user
  user_seen_movies={}
  for i in ratings.index:
    if ratings.loc[i,"userId"] in user_seen_movies:
      user_seen_movies[ratings.loc[i,"userId"]].append(ratings.loc[i,"movieId"])
    else:
      temp=[]
      temp.append(ratings.loc[i,"movieId"])
      user_seen_movies[ratings.loc[i,"userId"]]=temp

  #if the user has not seen the movie, append it to recommendation dictionary
  def predict2(userid, df_2, pred):
    recommendations={}
    j=0
    for i in df_2.index:
      if df_2.loc[i,"movieId"] not in user_seen_movies[userid]:
        recommendations[df_2.loc[i,"movieId"]]=pred[j]
      j+=1
    return recommendations

  #sort the dictionary in descending order
  recommendation_dict=predict2(3,df_2,pred)
  sorted_recommendation_dict=dict(sorted(recommendation_dict.items(), key=operator.itemgetter(1),reverse=True))
  list_keys=list(sorted_recommendation_dict.keys())

  #print the top 10 rated movies
  for i in range(10):
    idx=movies.index[movies["movieId"]==list_keys[i]].values
    idx=int(idx)
    print(movies.loc[idx,"title"])

predict_based_on_user_id(5)

"""#Pipeline"""

def pipeline(str):
  if(str=='New'):
    print('Top movies based on ratings and popularity are:')
    print(highly_rated_popular_movies[(highly_rated_popular_movies['Number of Users watched']>200) & (highly_rated_popular_movies['avg rating']>=4.0)][['title','avg rating']])
    print()
    

    print('Most popular movies of every genre are:')
    for i in unique_genres:
      recommendations_genre(i)

  else:
    userid=int(input('Please Enter User Id:'))
    movie_ip=input('Please enter a movie you have watched and that you like:')

    print()
    print('Recommendations using Correlation based on movie liked by user:')
    recc=prediction_using_correlation(movie_ip)
    print(recc[['title','genres']].head(10))
    print()
    
    print('Recommendations using Stochastic Gradient Descent based on movies the user has not watched:')
    already_rated,predictions=recommendation_using_ratings(userid)
    print(predictions.head(10))
    print()

    print("Recommendations using Collaborative Filtering and KNN based on the movie liked by the user:")
    predict_knn=collaborative_knn(movie_ip)
    print(predict_knn['Title'].head(10))
    print()

    print("Recommendations using Deep Learning Models based on the movie liked by the user as well as the top ratings:")
    predict_based_on_user_id(userid)    
    print()

user_type=input('Please enter New if you are a new user and Old if you are a returning user:')
pipeline(user_type)